{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting and Converting Articles about LoRa and LoRaWAN from The Things Network\n",
    "\n",
    "This notebook demonstrates the process of extracting and converting articles from The Things Network website into Markdown format and saving them locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "from markdownify import markdownify as md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Directories and Constants\n",
    "Setting up the URL of the host, base directory for saving articles, and delay between requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRa Host Page URL, base directory for saving articles, and delay between requests\n",
    "host = \"https://www.thethingsnetwork.org/\"\n",
    "base_dir = \"data/articles/\"\n",
    "delay_seconds = 4\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Article Page\n",
    "We need retrieves the article page from a given URL and returns the status code and response text as a tuple of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the article page from URL and return status code and response text as tuple of strings\n",
    "def get_article_page(\n",
    "    url: str,\n",
    "    delay_seconds: int = 30,\n",
    "    headers: Optional[dict[str, str]] = None,\n",
    "    encoding: str = \"utf-8\",\n",
    "    timeout: int = 30,\n",
    ") -> Tuple[str, str]:\n",
    "    if headers is None:\n",
    "        headers = {\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "            \"Accept-Language\": \"es-419,es;q=0.7\",\n",
    "            \"Cache-Control\": \"max-age=0\",\n",
    "            \"Cookie\": \"csrftoken=AmCKlYZ5ypxswQEXbhOJ04v7RRHCxk171Ldc8l3SqDhKC7PtvPfah5jQtsp8ITwQ\",\n",
    "            \"If-Modified-Since\": \"Tue, 25 Jun 2024 17:50:49 GMT\",\n",
    "            \"If-None-Match\": 'W/\"667b0379-73fd\"',\n",
    "            \"Priority\": \"u=0, i\",\n",
    "            \"Sec-Ch-Ua\": '\"Not/A)Brand\";v=\"8\", \"Chromium\";v=\"126\", \"Brave\";v=\"126\"',\n",
    "            \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "            \"Sec-Ch-Ua-Platform\": '\"macOS\"',\n",
    "            \"Sec-Fetch-Dest\": \"document\",\n",
    "            \"Sec-Fetch-Mode\": \"navigate\",\n",
    "            \"Sec-Fetch-Site\": \"none\",\n",
    "            \"Sec-Fetch-User\": \"?1\",\n",
    "            \"Sec-Gpc\": \"1\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\",\n",
    "        }\n",
    "    # Get Response from URL and return status code and response text\n",
    "    response = requests.get(url, headers=headers, timeout=timeout)\n",
    "    time.sleep(delay_seconds)\n",
    "    if encoding:\n",
    "        response.encoding = encoding\n",
    "    return response.status_code, response.text\n",
    "\n",
    "\n",
    "# Get Response from URL with raise exception if status code is not 200\n",
    "def get_response_from_url(url: str, delay_seconds: int = 30) -> str:\n",
    "    status_code, response = get_article_page(url, delay_seconds)\n",
    "    if status_code != 200:\n",
    "        raise Exception(f\"Failed to get response from {url}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Get URLs of Articles\n",
    "This function help to retrieves all article URLs from the host page and returns them in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URLs of the articles on the host page and save them to a array\n",
    "def get_articles_url(base_url):\n",
    "    response = get_response_from_url(base_url, delay_seconds)\n",
    "    soup = BeautifulSoup(response, \"html.parser\")\n",
    "    menu_container = soup.find(\"ul\", class_=\"menu-list\")\n",
    "    link_containers = menu_container.find_all(\"li\")\n",
    "    urls = [\n",
    "        urljoin(base_url, a[\"href\"])\n",
    "        for link_container in link_containers\n",
    "        for a in link_container.find_all(\"a\")\n",
    "    ]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.thethingsnetwork.org/docs/lorawan/what-is-lorawan/', 'https://www.thethingsnetwork.org/docs/lorawan/architecture/', 'https://www.thethingsnetwork.org/docs/lorawan/regional-parameters/', 'https://www.thethingsnetwork.org/docs/lorawan/lorawan-relay/', 'https://www.thethingsnetwork.org/docs/lorawan/message-types/', 'https://www.thethingsnetwork.org/docs/lorawan/security/', 'https://www.thethingsnetwork.org/docs/lorawan/classes/', 'https://www.thethingsnetwork.org/docs/lorawan/end-device-activation/', 'https://www.thethingsnetwork.org/docs/lorawan/spreading-factors/', 'https://www.thethingsnetwork.org/docs/lorawan/adaptive-data-rate/', 'https://www.thethingsnetwork.org/docs/lorawan/limitations/', 'https://www.thethingsnetwork.org/docs/lorawan/frequencies-by-country/', 'https://www.thethingsnetwork.org/docs/lorawan/frequency-plans/', 'https://www.thethingsnetwork.org/docs/lorawan/duty-cycle/', 'https://www.thethingsnetwork.org/docs/lorawan/glossary/', 'https://www.thethingsnetwork.org/docs/lorawan/modulation-data-rate/', 'https://www.thethingsnetwork.org/docs/lorawan/addressing/', 'https://www.thethingsnetwork.org/docs/lorawan/academic/', 'https://www.thethingsnetwork.org/docs/lorawan/antenna-connectors/', 'https://www.thethingsnetwork.org/docs/lorawan/antenna-types/', 'https://www.thethingsnetwork.org/docs/lorawan/db-dbm-dbi-dbd/', 'https://www.thethingsnetwork.org/docs/lorawan/eirp-and-erp/', 'https://www.thethingsnetwork.org/docs/lorawan/fec-and-code-rate/', 'https://www.thethingsnetwork.org/docs/lorawan/lora-phy-format/', 'https://www.thethingsnetwork.org/docs/lorawan/concentrators/', 'https://www.thethingsnetwork.org/docs/lorawan/transceivers/', 'https://www.thethingsnetwork.org/docs/lorawan/prefix-assignments/', 'https://www.thethingsnetwork.org/docs/lorawan/preparing-certification/', 'https://www.thethingsnetwork.org/docs/lorawan/regional-limitations-of-rf-use/', 'https://www.thethingsnetwork.org/docs/lorawan/rssi-and-snr/', 'https://www.thethingsnetwork.org/docs/lorawan/the-things-certified-security/']\n"
     ]
    }
   ],
   "source": [
    "# Get URLs from the host page and save them to an array\n",
    "dir_url = f\"{host}docs/lorawan/what-is-lorawan/\"\n",
    "urls = get_articles_url(dir_url)\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean HTML content\n",
    "We need to clean the HTML content that we have extracted so that we can only have the information necessary for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the HTML\n",
    "def clean_html(soup):\n",
    "    content_container = soup.find(\"div\", class_=\"content\")\n",
    "    if content_container:\n",
    "        # Remove images, header hashes, figure descriptions, iframes, questions, and paragraphs from the content\n",
    "        for img_content in content_container.find_all(\"img\"):\n",
    "            img_content.decompose()\n",
    "        for header_hash in content_container.find_all(\"a\", class_=\"header-hash\"):\n",
    "            header_hash.decompose()\n",
    "        for figure_description in content_container.find_all(\"em\"):\n",
    "            figure_description.decompose()\n",
    "        iframes = content_container.find_all(\"iframe\")\n",
    "        for iframe in iframes:\n",
    "            src = iframe.get(\"src\")\n",
    "            iframe.replace_with(f\"Video: {src}\\n\")\n",
    "        question_title = content_container.find(id=\"questions\")\n",
    "        if question_title:\n",
    "            question_title.decompose()\n",
    "        question_container = content_container.find(\"ol\")\n",
    "        if question_container:\n",
    "            for question in question_container.find_all(\"li\"):\n",
    "                question.decompose()\n",
    "\n",
    "        # Get the text from the paragraphs and replace the paragraphs with the text\n",
    "        paragraphs = content_container.find_all(\"p\")\n",
    "        for paragraph in paragraphs:\n",
    "            new_text = paragraph.get_text(\" \", strip=True)  # Add space between words\n",
    "            paragraph.replace_with(new_text)\n",
    "\n",
    "    # Clean title text and replace with markdown header\n",
    "    title = soup.find(\"h1\", class_=\"title\")\n",
    "    if title:\n",
    "        title_text = title.get_text(\" \", strip=True)\n",
    "        title.replace_with(f\"# {title_text}\\n\")\n",
    "\n",
    "    # Remove the clearfix containers\n",
    "    clearfix_containers = soup.find_all(\"div\", class_=\"is-clearfix\")\n",
    "    for clearfix_container in clearfix_containers:\n",
    "        clearfix_container.decompose()\n",
    "\n",
    "    # Return the cleaned text\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert HTML content to Markdown and extract the information\n",
    "Now we need to convert the HTML content to Markdown format using the Markdownify library. First we will create the function for the conversion and then we will extract the information from the web page and save it in \"md\" files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert HTML content to Markdown\n",
    "def convert_to_markdown(html_content):\n",
    "    markdown_text = md(html_content, heading_style=\"ATX\")\n",
    "    markdown_text = re.sub(r'\\n{3,}', '\\n\\n', markdown_text)\n",
    "    markdown_text = re.sub(r' +\\n', '\\n', markdown_text)\n",
    "    markdown_text = markdown_text.strip()\n",
    "    \n",
    "    return markdown_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract information from the URL\n",
    "def extract_information(url):\n",
    "    try:\n",
    "        response = get_response_from_url(url, delay_seconds)\n",
    "        soup = BeautifulSoup(response, \"html.parser\")\n",
    "        content = soup.find(\"div\", class_=\"docs-content\")\n",
    "        if content:\n",
    "            cleaned_content = clean_html(content)\n",
    "            markdown_content = convert_to_markdown(str(cleaned_content ))\n",
    "            return markdown_content\n",
    "        return \"Error: No content found\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the content to a markdown file\n",
    "def create_markdown_file(content, filename):\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information from the URLs and save them to markdown files\n",
    "for idx, url in enumerate(urls):\n",
    "    content = extract_information(url)\n",
    "    url_title = url.split(\"/\")[-2]\n",
    "    path = f\"{base_dir}{idx+1}-{url_title}.md\"\n",
    "    if os.path.exists(path):\n",
    "        continue\n",
    "    print(path)\n",
    "    create_markdown_file(content, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path of the markdown file where all content will be stored\n",
    "combined_file_path = f\"data/articles/lorawan-information.md\"\n",
    "\n",
    "# Open the file in append mode\n",
    "with open(combined_file_path, \"w\") as file:\n",
    "    # Extract information from the URLs and append them to the markdown file\n",
    "    for url in urls:\n",
    "        content = extract_information(url)\n",
    "        # Remove leading backslash from content\n",
    "        content = content.lstrip(\"\\\\\")\n",
    "        file.write(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lorawan-chatbot-llm-JqNepD_O-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
